ğŸ§® LLM-SRBench: Symbolic Regression Benchmarking with LLMs

A comparative evaluation of modern Large Language Models and evolutionary Symbolic Regression techniques

ğŸ“Œ Overview

This project implements a complete symbolic regression benchmarking pipeline, inspired by the research paper:
â€œLLM-SRBench: A New Benchmark for Evaluating LLMs on Symbolic Regressionâ€ (ICML 2025 submission)
The goal is to evaluate how different reasoning methods and LLMs recover underlying mathematical expressions from small numeric samples.

You benchmark:

Models
TinyLlama-1.1B-Chat
Phi-2 (Microsoft)
GPT-2 XL

Methods
Direct prompting
Chain-of-Thought (CoT) prompting
LLMSR (LLM-guided symbolic regression, your custom implementation)
LaSR-v2 (a pure-Python evolutionary symbolic regression engine)

Datasets
Synthetic equations (20 equations auto-generated)
Transform equations (physics & math equations from ltransform dataset)

All evaluations are stored in results.csv, and visualized using multiple analytical plots.

ğŸ† Key Features

âœ” 1. Per-model dynamic quantized loading

FP16 for small models
8-bit quantization + CPU offload (bitsandbytes) for larger models
Automatically frees VRAM after each model run

âœ” 2. Fully reproducible symbolic regression evaluation

Each method returns a Python Î»-function which is evaluated numerically and symbolically:
NMSE (Normalized MSE)
acc@0.1 (accuracy within 0.1 error)
boolean symbolic equivalence

âœ” 3. Custom LLM-SR Implementation

Your pipeline includes a real LLMSR algorithm, combining:
evolutionary search
numerical curve fitting
local optimization
LLM-based prior proposal

âœ” 4. LaSR-v2 (your custom genetic symbolic regressor)

Lightweight pure-Python SGA symbolic regression engine with:
mutation
crossover
elitism
numerical constant tuning
expression simplification

âœ” 5. End-to-end Benchmarking & Visualization
You generate:
Model Ã— Dataset accuracy
Method Ã— Model accuracy
Method Ã— Dataset accuracy
Method Ã— Dataset Ã— Model (3-axis comparison)
Comparison vs paper benchmark values
All plots are formatted for publication-level clarity.


ğŸ“Š Final Results Summary

ğŸ¥‡ Best Method (overall)
Method	Accuracy (acc@0.1)
LLMSR	â‰ˆ 66%
LaSR-v2	46.6%
Direct	~5%
CoT	~1%

ğŸ¥‡ Best Model (overall)
Model	Accuracy
TinyLlama-1.1B	Highest overall accuracy
Phi-2	moderate
GPT-2 XL	close behind

ğŸ“Œ Synthetic vs Transform Dataset
LLMSR excels on synthetic equations (81%),
Drops significantly on transform equations (6â€“7%) due to multivariable physics formulas.
LaSR scores 100% on transform for some models (trivial expressions), but lower on synthetic.

ğŸ†š Paper Comparison
Method	           Paper Accuracy	  My Implementation
LLMSR                 ~67%	                ~66%
LaSR	              ~63%	                ~47%

Implementation slightly matches paper results because:

Simpler datasets was used
Synthetic equations are easier to recover
Applied numerical tuning + simplification


ğŸ§¬ Project Structure
LLMSR-bench/
â”‚
â”œâ”€â”€ final.py                # Main benchmark runner
â”‚
â”‚
â”œâ”€â”€ methods/
â”‚   â”œâ”€â”€ direct/             # Direct prompting
â”‚   â”œâ”€â”€ cot/                # Chain of Thought prompting
â”‚   â”œâ”€â”€ llmsr/              # LLMSR engine (optimizers + LLM proposals)
â”‚   â”œâ”€â”€ lasr/lasr.py        # LaSR-v2 evolutionary regressor
â”‚
â”œâ”€â”€ bench/
â”‚   â”œâ”€â”€ evaluator.py        # Accuracy evaluation utilities
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ ltransform_dataset.json   # Transform dataset (5 equations used)
â”‚
â”œâ”€â”€ results.csv             # Final benchmark results
â”œâ”€â”€ requirements.txt        # Environment dependencies
â””â”€â”€ README.md




ğŸ“š Reference

This work is inspired by:
LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models by
Parshin Shojaee, Ngoc-Hieu Nguyen, Kazem Meidani, Amir Barati Farimani, Khoa D Doan, Chandan K. Reddy
ICML 2025
